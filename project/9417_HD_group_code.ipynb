{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "teamname = 'hd-group-unsw'\n",
    "data_folder='s3://tf-trachack-data/212/'\n",
    "root_folder='s3://tf-trachack-notebooks/'+teamname+'/jupyter/jovyan/'\n",
    "\n",
    "# import all csv data\n",
    "upgrades=pd.read_csv(data_folder+\"data/dev/upgrades.csv\")\n",
    "customer_info=pd.read_csv(data_folder+\"data/dev/customer_info.csv\")\n",
    "phone_info=pd.read_csv(data_folder+\"data/dev/phone_info.csv\")\n",
    "redemptions=pd.read_csv(data_folder+\"data/dev/redemptions.csv\")\n",
    "deactivations=pd.read_csv(data_folder+\"data/dev/deactivations.csv\")\n",
    "reactivations=pd.read_csv(data_folder+\"data/dev/reactivations.csv\")\n",
    "suspensions=pd.read_csv(data_folder+\"data/dev/suspensions.csv\")\n",
    "network_usage_domestic=pd.read_csv(data_folder+\"data/dev/network_usage_domestic.csv\")\n",
    "lrp_points=pd.read_csv(data_folder+\"data/dev/lrp_points.csv\")\n",
    "lrp_enrollment=pd.read_csv(data_folder+\"data/dev/lrp_enrollment.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process upgrades and customer_info\n",
    "up = upgrades.copy(deep = True)\n",
    "up['upgrade'].replace({'yes':1,'no':0}, inplace = True)\n",
    "customer = customer_info.copy(deep = True)\n",
    "# transform time from str type to int type\n",
    "first_activation_list = np.array(customer['first_activation_date'])\n",
    "for i in range(len(first_activation_list)):\n",
    "    if first_activation_list[i]==first_activation_list[i]:\n",
    "        t = first_activation_list[i]\n",
    "        t = time.strptime(t, \"%Y-%M-%d\")\n",
    "        t = time.mktime(t)\n",
    "        first_activation_list[i] = t\n",
    "customer['first_activation_date'] = first_activation_list\n",
    "\n",
    "redemption_list = np.array(customer['redemption_date'])\n",
    "for i in range(len(redemption_list)):\n",
    "    if redemption_list[i] == redemption_list[i]:\n",
    "        t1 = redemption_list[i]\n",
    "        t1 = time.strptime(t1, \"%Y-%M-%d\")\n",
    "        t1 = time.mktime(t1)\n",
    "        redemption_list[i] = t1\n",
    "customer['redemption_date'] = redemption_list\n",
    "\n",
    "# normalisation for time\n",
    "max_v = np.max(customer.loc[:, 'first_activation_date'])\n",
    "min_v = np.min(customer.loc[:, 'first_activation_date'])\n",
    "customer.loc[:,'first_activation_date'] = (customer.loc[:,'first_activation_date']-min_v)/(max_v-min_v)\n",
    "\n",
    "max_v = np.max(customer['redemption_date'])\n",
    "min_v = np.min(customer['redemption_date'])\n",
    "customer.loc[:,'redemption_date'] = (customer.loc[:,'redemption_date']-min_v)/(max_v-min_v)\n",
    "\n",
    "customer['carrier'].replace({'carrier 1':1, 'carrier 2': 2, 'carrier 3': 3},inplace = True)\n",
    "customer.drop(columns = 'plan_subtype',inplace = True)\n",
    "# fill NAN with np.mean\n",
    "customer['first_activation_date'].fillna(value=np.mean(customer['first_activation_date']), inplace=True)\n",
    "\n",
    "customer['plan_name'].replace({'Other':0, 'plan 1':1, 'plan 2': 2, 'plan 3': 3,'plan 4':4},inplace = True )\n",
    "\n",
    "customer['plan_name'].fillna(value=-1, inplace=True)\n",
    "customer['redemption_date'].fillna(value=-1, inplace=True)\n",
    "# merge two tables\n",
    "train_data = pd.merge(up,customer,how='inner',on='line_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select some features from phone_info\n",
    "# transform data into numeric and process NAN\n",
    "selected_phone_info = phone_info.loc[:,['line_id','cpu_cores','expandable_storage','gsma_device_type','lte'\n",
    "                                        ,'lte_advanced','os_name','sim_size','touch_screen','wi_fi','year_released']]\n",
    "selected_phone_info['cpu_cores'].replace({'1+2+4':1,'1':2,'2+2+4':3,'4+2':4,'1+3+4':5,'2+2':6,\n",
    "                          '6+2':7,'2+6':8,'4+4':9,'8':10,'2+4':11,'2':12,'4':13}, inplace = True)\n",
    "selected_phone_info['gsma_device_type'].replace({'Mobile Phone/Feature phone':1,'Handheld':2, 'Smartphone':3,'WLAN Router':4, 'Modem': 5, 'Tablet': 6}, inplace = True)\n",
    "selected_phone_info['os_name'].replace({'Windows Phone':1,'Android':2, 'iOS':3,'KaiOS':4, 'LG proprietary': 5, 'Nucleus': 6,'Samsung proprietary':7,'Nokia OS':8,'Other':9}, inplace = True)\n",
    "selected_phone_info['sim_size'].replace({'Micro & Mini':1,'Micro & Nano':2,'Micro':3,'Nano':4,'Mini': 5}, inplace = True)\n",
    "selected_phone_info['cpu_cores'].fillna(value=-1, inplace=True)\n",
    "selected_phone_info['expandable_storage'].fillna(value=-1, inplace=True)\n",
    "selected_phone_info['gsma_device_type'].fillna(value=-1, inplace=True)\n",
    "selected_phone_info['lte'].fillna(value=-1, inplace=True)\n",
    "selected_phone_info['lte_advanced'].fillna(value=-1, inplace=True)\n",
    "selected_phone_info['os_name'].fillna(value=-1, inplace=True)\n",
    "selected_phone_info['sim_size'].fillna(value=-1, inplace=True)\n",
    "selected_phone_info['touch_screen'].fillna(value=-1, inplace=True)\n",
    "selected_phone_info['wi_fi'].fillna(value=-1, inplace=True)\n",
    "selected_phone_info['year_released'].fillna(value=-1, inplace=True)\n",
    "train_data = pd.merge(train_data,selected_phone_info,how='inner',on='line_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process network_usage_domestic\n",
    "network = network_usage_domestic.loc[:,['line_id','mms_in','mms_out','sms_in','sms_out','total_kb','voice_count_total']]\n",
    "network = network.groupby('line_id').sum()\n",
    "train_data = pd.merge(train_data,network,on='line_id',how='left')\n",
    "max_v = np.max(train_data.loc[:, 'total_kb'])\n",
    "min_v = np.min(train_data.loc[:, 'total_kb'])\n",
    "train_data.loc[:,'total_kb'] = (train_data.loc[:,'total_kb']-min_v)/(max_v-min_v)\n",
    "train_data['mms_in'].fillna(value=-1, inplace=True)\n",
    "train_data['mms_out'].fillna(value=-1, inplace=True)\n",
    "train_data['sms_in'].fillna(value=-1, inplace=True)\n",
    "train_data['sms_out'].fillna(value=-1, inplace=True)\n",
    "train_data['total_kb'].fillna(value=-1, inplace=True)\n",
    "train_data['voice_count_total'].fillna(value=-1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process redemption table\n",
    "# picking redemption_times and gross_revenue as features\n",
    "redem = redemptions.groupby('line_id').sum()\n",
    "redem.reset_index()\n",
    "redemption_times = []\n",
    "id_times = {}\n",
    "times = redemptions['line_id'].value_counts()\n",
    "userid = times.index.tolist()\n",
    "for i in range(len(userid)):\n",
    "    id_times[userid[i]] = times[userid[i]]\n",
    "userid_redemptions = upgrades['line_id'].tolist()\n",
    "for i in range(len(userid_redemptions)):\n",
    "    if userid_redemptions[i] in id_times.keys():\n",
    "        redemption_times.append(times[userid_redemptions[i]])\n",
    "    else:\n",
    "        redemption_times.append(0)\n",
    "        \n",
    "train_data = pd.merge(train_data, redem, on='line_id',how='left')\n",
    "train_data['redemption_times']=redemption_times\n",
    "train_data['gross_revenue'].fillna(value = 0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deactivation table\n",
    "line_id = upgrades['line_id'].tolist()\n",
    "line_id_deact = set(deactivations['line_id'].tolist())\n",
    "latest_deactivation_date = []\n",
    "latest_deactivation_date_dict = {}\n",
    "deact_times = []\n",
    "deact_times_dict={}\n",
    "# obtain times and latest_deactivation_date\n",
    "for i in range(len(line_id)):\n",
    "    if line_id[i] in line_id_deact:\n",
    "        info = deactivations[deactivations['line_id']==line_id[i]]\n",
    "        date = info['deactivation_date'].tolist()\n",
    "        for j in range(len(date)):\n",
    "            t = time.strptime(date[j], \"%Y-%M-%d\")\n",
    "            t = time.mktime(t)\n",
    "            date[j] = t\n",
    "        latest_deactivation_date.append(np.max(date))\n",
    "        deact_times.append(len(date))\n",
    "    else:\n",
    "        latest_deactivation_date.append(0)\n",
    "        deact_times.append(0)\n",
    "\n",
    "train_data['latest_deactivation_date'] = latest_deactivation_date\n",
    "train_data['deact_times'] = deact_times\n",
    "# normalisation\n",
    "max_v = np.max(train_data.loc[:, 'latest_deactivation_date'])\n",
    "min_v = np.min(train_data.loc[:, 'latest_deactivation_date'])\n",
    "train_data.loc[:,'latest_deactivation_date'] = (train_data.loc[:,'latest_deactivation_date']-min_v)/(max_v-min_v)\n",
    "# one-hot encoder for deactivation_reason\n",
    "deact_dummy = pd.get_dummies(deactivations,columns=['deactivation_reason'])\n",
    "deact_dummy = deact_dummy.loc[:,['line_id','deactivation_date','deactivation_reason_ACTIVE UPGRADE','deactivation_reason_UPGRADE','deactivation_reason_PASTDUE']]\n",
    "deact_dummy = deact_dummy.groupby('line_id')['deactivation_reason_ACTIVE UPGRADE','deactivation_reason_UPGRADE','deactivation_reason_PASTDUE'].sum().reset_index()\n",
    "train_data = pd.merge(train_data, deact_dummy, on='line_id',how='left')\n",
    "train_data['deactivation_reason_ACTIVE UPGRADE'].fillna(value = 0, inplace = True)\n",
    "train_data['deactivation_reason_UPGRADE'].fillna(value=0, inplace=True)\n",
    "train_data['deactivation_reason_PASTDUE'].fillna(value=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reacitivation\n",
    "line_id = upgrades['line_id'].tolist()\n",
    "line_id_deact = set(reactivations['line_id'].tolist())\n",
    "latest_reactivations_date = []\n",
    "react_times = []\n",
    "\n",
    "# obtain times and latest_reactivations_date\n",
    "for i in range(len(line_id)):\n",
    "    if line_id[i] in line_id_deact:\n",
    "        info = reactivations[reactivations['line_id']==line_id[i]]\n",
    "        date = info['reactivation_date'].tolist()\n",
    "        for j in range(len(date)):\n",
    "            t = time.strptime(date[j], \"%Y-%M-%d\")\n",
    "            t = time.mktime(t)\n",
    "            date[j] = t\n",
    "        latest_reactivations_date.append(np.max(date))\n",
    "        react_times.append(len(date))\n",
    "    else:\n",
    "        latest_reactivations_date.append(0)\n",
    "        react_times.append(0)\n",
    "# normalisation\n",
    "train_data['latest_reactivations_date'] = latest_reactivations_date\n",
    "train_data['react_times'] = react_times\n",
    "max_v = np.max(train_data.loc[:, 'latest_reactivations_date'])\n",
    "min_v = np.min(train_data.loc[:, 'latest_reactivations_date'])\n",
    "train_data.loc[:,'latest_reactivations_date'] = (train_data.loc[:,'latest_reactivations_date']-min_v)/(max_v-min_v)\n",
    "\n",
    "# suspensions\n",
    "line_id = upgrades['line_id'].tolist()\n",
    "line_id_sus = set(suspensions['line_id'].tolist())\n",
    "sus_times = []\n",
    "\n",
    "# obtain suspension times\n",
    "for i in range(len(line_id)):\n",
    "    if line_id[i] in line_id_sus:\n",
    "        info = suspensions[suspensions['line_id']==line_id[i]]\n",
    "        date = info['suspension_start_date'].tolist()\n",
    "        sus_times.append(len(date))\n",
    "    else:\n",
    "        sus_times.append(0)\n",
    "train_data['sus_times'] = sus_times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lrp_points\n",
    "line_id = upgrades['line_id'].tolist()\n",
    "line_id_lrp = set(lrp_points['line_id'].tolist())\n",
    "total_quantity = []\n",
    "lrp_points.dropna(axis = 0, subset = ['total_quantity'], inplace = True)\n",
    "\n",
    "for i in range(len(line_id)):\n",
    "    if line_id[i] in line_id_lrp:\n",
    "        info = lrp_points[lrp_points['line_id']==line_id[i]]\n",
    "        total_quantity.append(info['total_quantity'].tolist()[0])\n",
    "    else:\n",
    "        total_quantity.append(0)\n",
    "\n",
    "train_data['total_quantity'] = total_quantity\n",
    "\n",
    "# lrp_enrolments\n",
    "lrp_enrollment\n",
    "line_id_lrp_enrol = lrp_enrollment['line_id'].tolist()\n",
    "line_id_lrp_enrol = list(set(line_id_lrp_enrol))\n",
    "lrp_enrolled = []\n",
    "for i in range(len(line_id)):\n",
    "    if line_id[i] in line_id_lrp_enrol:\n",
    "        lrp_enrolled.append(1)\n",
    "    else:\n",
    "        lrp_enrolled.append(0)\n",
    "        \n",
    "train_data['lrp_enrolled'] = lrp_enrolled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split for training dataset\n",
    "df_Y = train_data['upgrade']\n",
    "df_X = train_data.drop(columns=['line_id','date_observed','upgrade'])\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(df_X, df_Y, test_size=0.3, random_state=511)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import eval tables\n",
    "upgrades_eval=pd.read_csv(data_folder+\"data/eval/upgrades.csv\")\n",
    "customer_info_eval=pd.read_csv(data_folder+\"data/eval/customer_info.csv\")\n",
    "phone_info_eval=pd.read_csv(data_folder+\"data/eval/phone_info.csv\")\n",
    "redemptions_eval=pd.read_csv(data_folder+\"data/eval/redemptions.csv\")\n",
    "deactivations_eval=pd.read_csv(data_folder+\"data/eval/deactivations.csv\")\n",
    "reactivations_eval=pd.read_csv(data_folder+\"data/eval/reactivations.csv\")\n",
    "suspensions_eval=pd.read_csv(data_folder+\"data/eval/suspensions.csv\")\n",
    "network_usage_domestic_eval=pd.read_csv(data_folder+\"data/eval/network_usage_domestic.csv\")\n",
    "lrp_points_eval=pd.read_csv(data_folder+\"data/eval/lrp_points.csv\")\n",
    "lrp_enrollment_eval=pd.read_csv(data_folder+\"data/eval/lrp_enrollment.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same process as training data\n",
    "# eval data\n",
    "up_eval = upgrades_eval.copy(deep = True)\n",
    "\n",
    "customer_eval = customer_info_eval.copy(deep = True)\n",
    "# 将str时间类型 改为float类型\n",
    "first_activation_list = np.array(customer_eval['first_activation_date'])\n",
    "for i in range(len(first_activation_list)):\n",
    "    if first_activation_list[i]==first_activation_list[i]:\n",
    "        t = first_activation_list[i]\n",
    "        t = time.strptime(t, \"%Y-%M-%d\")\n",
    "        t = time.mktime(t)\n",
    "        first_activation_list[i] = t\n",
    "customer_eval['first_activation_date'] = first_activation_list\n",
    "\n",
    "redemption_list = np.array(customer_eval['redemption_date'])\n",
    "for i in range(len(redemption_list)):\n",
    "    if redemption_list[i] == redemption_list[i]:\n",
    "        t1 = redemption_list[i]\n",
    "        t1 = time.strptime(t1, \"%Y-%M-%d\")\n",
    "        t1 = time.mktime(t1)\n",
    "        redemption_list[i] = t1\n",
    "customer_eval['redemption_date'] = redemption_list\n",
    "\n",
    "# 归一化\n",
    "max_v = np.max(customer_eval.loc[:, 'first_activation_date'])\n",
    "min_v = np.min(customer_eval.loc[:, 'first_activation_date'])\n",
    "customer_eval.loc[:,'first_activation_date'] = (customer_eval.loc[:,'first_activation_date']-min_v)/(max_v-min_v)\n",
    "\n",
    "max_v = np.max(customer_eval['redemption_date'])\n",
    "min_v = np.min(customer_eval['redemption_date'])\n",
    "customer_eval.loc[:,'redemption_date'] = (customer_eval.loc[:,'redemption_date']-min_v)/(max_v-min_v)\n",
    "\n",
    "customer_eval['carrier'].replace({'carrier 1':1, 'carrier 2': 2, 'carrier 3': 3},inplace = True)\n",
    "customer_eval.drop(columns = 'plan_subtype',inplace = True)\n",
    "customer_eval['first_activation_date'].fillna(value=np.mean(customer_eval['first_activation_date']), inplace=True)\n",
    "customer_eval['plan_name'].replace({'Other':0, 'plan 1':1, 'plan 2': 2, 'plan 3': 3,'plan 4':4},inplace = True )\n",
    "customer_eval['plan_name'].fillna(value=-1, inplace=True)\n",
    "customer_eval['redemption_date'].fillna(value=-1, inplace=True)\n",
    "\n",
    "eval_data = pd.merge(up_eval,customer_eval,how='inner',on='line_id')\n",
    "\n",
    "selected_phone_info = phone_info_eval.loc[:,['line_id','cpu_cores','expandable_storage','gsma_device_type','lte'\n",
    "                                        ,'lte_advanced','os_name','sim_size','touch_screen','wi_fi','year_released']]\n",
    "selected_phone_info['cpu_cores'].replace({'1+2+4':1,'1':2,'2+2+4':3,'4+2':4,'1+3+4':5,'2+2':6,\n",
    "                          '6+2':7,'2+6':8,'4+4':9,'8':10,'2+4':11,'2':12,'4':13}, inplace = True)\n",
    "selected_phone_info['gsma_device_type'].replace({'Mobile Phone/Feature phone':1,'Handheld':2, 'Smartphone':3,'WLAN Router':4, 'Modem': 5, 'Tablet': 6}, inplace = True)\n",
    "selected_phone_info['os_name'].replace({'Windows Phone':1,'Android':2, 'iOS':3,'KaiOS':4, 'LG proprietary': 5, 'Nucleus': 6,'Samsung proprietary':7,'Nokia OS':8,'Other':9}, inplace = True)\n",
    "selected_phone_info['sim_size'].replace({'Micro & Mini':1,'Micro & Nano':2,'Micro':3,'Nano':4,'Mini': 5}, inplace = True)\n",
    "selected_phone_info['cpu_cores'].fillna(value=-1, inplace=True)\n",
    "selected_phone_info['expandable_storage'].fillna(value=-1, inplace=True)\n",
    "selected_phone_info['gsma_device_type'].fillna(value=-1, inplace=True)\n",
    "selected_phone_info['lte'].fillna(value=-1, inplace=True)\n",
    "selected_phone_info['lte_advanced'].fillna(value=-1, inplace=True)\n",
    "selected_phone_info['os_name'].fillna(value=-1, inplace=True)\n",
    "selected_phone_info['sim_size'].fillna(value=-1, inplace=True)\n",
    "selected_phone_info['touch_screen'].fillna(value=-1, inplace=True)\n",
    "selected_phone_info['wi_fi'].fillna(value=-1, inplace=True)\n",
    "selected_phone_info['year_released'].fillna(value=-1, inplace=True)\n",
    "\n",
    "eval_data = pd.merge(eval_data,selected_phone_info,how='inner',on='line_id')\n",
    "\n",
    "network_eval = network_usage_domestic_eval.loc[:,['line_id','mms_in','mms_out','sms_in','sms_out','total_kb','voice_count_total']]\n",
    "network_eval = network_eval.groupby('line_id').sum()\n",
    "eval_data = pd.merge(eval_data,network_eval,on='line_id',how='left')\n",
    "max_v = np.max(eval_data.loc[:, 'total_kb'])\n",
    "min_v = np.min(eval_data.loc[:, 'total_kb'])\n",
    "eval_data.loc[:,'total_kb'] = (eval_data.loc[:,'total_kb']-min_v)/(max_v-min_v)\n",
    "eval_data['mms_in'].fillna(value=-1, inplace=True)\n",
    "eval_data['mms_out'].fillna(value=-1, inplace=True)\n",
    "eval_data['sms_in'].fillna(value=-1, inplace=True)\n",
    "eval_data['sms_out'].fillna(value=-1, inplace=True)\n",
    "eval_data['total_kb'].fillna(value=-1, inplace=True)\n",
    "eval_data['voice_count_total'].fillna(value=-1, inplace=True)\n",
    "\n",
    "redem = redemptions_eval.groupby('line_id').sum()\n",
    "redem.reset_index()\n",
    "redemption_times = []\n",
    "id_times = {}\n",
    "times = redemptions_eval['line_id'].value_counts()\n",
    "userid = times.index.tolist()\n",
    "for i in range(len(userid)):\n",
    "    id_times[userid[i]] = times[userid[i]]\n",
    "userid_redemptions = upgrades_eval['line_id'].tolist()\n",
    "for i in range(len(userid_redemptions)):\n",
    "    if userid_redemptions[i] in id_times.keys():\n",
    "        redemption_times.append(times[userid_redemptions[i]])\n",
    "    else:\n",
    "        redemption_times.append(0)\n",
    "eval_data = pd.merge(eval_data, redem, on='line_id',how='left')\n",
    "eval_data['redemption_times']=redemption_times\n",
    "eval_data['gross_revenue'].fillna(value = 0,inplace=True)\n",
    "\n",
    "# deactivation\n",
    "line_id = upgrades_eval['line_id'].tolist()\n",
    "line_id_deact = set(deactivations_eval['line_id'].tolist())\n",
    "latest_deactivation_date = []\n",
    "\n",
    "deact_times = []\n",
    "for i in range(len(line_id)):\n",
    "    if line_id[i] in line_id_deact:\n",
    "        info = deactivations_eval[deactivations_eval['line_id']==line_id[i]]\n",
    "        date = info['deactivation_date'].tolist()\n",
    "        for j in range(len(date)):\n",
    "            t = time.strptime(date[j], \"%Y-%M-%d\")\n",
    "            t = time.mktime(t)\n",
    "            date[j] = t\n",
    "        latest_deactivation_date.append(np.max(date))\n",
    "        deact_times.append(len(date))\n",
    "    else:\n",
    "        latest_deactivation_date.append(0)\n",
    "        deact_times.append(0)\n",
    "        \n",
    "eval_data['latest_deactivation_date'] = latest_deactivation_date\n",
    "eval_data['deact_times'] = deact_times\n",
    "\n",
    "max_v = np.max(eval_data.loc[:, 'latest_deactivation_date'])\n",
    "min_v = np.min(eval_data.loc[:, 'latest_deactivation_date'])\n",
    "eval_data.loc[:,'latest_deactivation_date'] = (eval_data.loc[:,'latest_deactivation_date']-min_v)/(max_v-min_v)\n",
    "deact_dummy = pd.get_dummies(deactivations_eval,columns=['deactivation_reason'])\n",
    "deact_dummy = deact_dummy.loc[:,['line_id','deactivation_date','deactivation_reason_ACTIVE UPGRADE','deactivation_reason_UPGRADE','deactivation_reason_PASTDUE']]\n",
    "deact_dummy = deact_dummy.groupby('line_id')['deactivation_reason_ACTIVE UPGRADE','deactivation_reason_UPGRADE','deactivation_reason_PASTDUE'].sum().reset_index()\n",
    "eval_data = pd.merge(eval_data, deact_dummy, on='line_id',how='left')\n",
    "eval_data['deactivation_reason_ACTIVE UPGRADE'].fillna(value = 0, inplace = True)\n",
    "eval_data['deactivation_reason_UPGRADE'].fillna(value=0, inplace=True)\n",
    "eval_data['deactivation_reason_PASTDUE'].fillna(value=0, inplace=True)\n",
    "\n",
    "# reactivation\n",
    "line_id_deact = set(reactivations_eval['line_id'].tolist())\n",
    "latest_reactivations_date = []\n",
    "react_times = []\n",
    "\n",
    "\n",
    "for i in range(len(line_id)):\n",
    "    if line_id[i] in line_id_deact:\n",
    "        info = reactivations_eval[reactivations_eval['line_id']==line_id[i]]\n",
    "        date = info['reactivation_date'].tolist()\n",
    "        for j in range(len(date)):\n",
    "            t = time.strptime(date[j], \"%Y-%M-%d\")\n",
    "            t = time.mktime(t)\n",
    "            date[j] = t\n",
    "        latest_reactivations_date.append(np.max(date))\n",
    "        react_times.append(len(date))\n",
    "    else:\n",
    "        latest_reactivations_date.append(0)\n",
    "        react_times.append(0)\n",
    "\n",
    "eval_data['latest_reactivations_date'] = latest_reactivations_date\n",
    "eval_data['react_times'] = react_times        \n",
    "max_v = np.max(eval_data.loc[:, 'latest_reactivations_date'])\n",
    "min_v = np.min(eval_data.loc[:, 'latest_reactivations_date'])\n",
    "eval_data.loc[:,'latest_reactivations_date'] = (eval_data.loc[:,'latest_reactivations_date']-min_v)/(max_v-min_v)\n",
    "\n",
    "# suspensions\n",
    "line_id_sus = set(suspensions_eval['line_id'].tolist())\n",
    "sus_times = []\n",
    "\n",
    "\n",
    "for i in range(len(line_id)):\n",
    "    if line_id[i] in line_id_sus:\n",
    "        info = suspensions_eval[suspensions_eval['line_id']==line_id[i]]\n",
    "        date = info['suspension_start_date'].tolist()\n",
    "        sus_times.append(len(date))\n",
    "    else:\n",
    "        sus_times.append(0)\n",
    "        \n",
    "eval_data['sus_times'] = sus_times\n",
    "\n",
    "# lrp_points\n",
    "lrp_eval = lrp_points_eval.dropna(axis = 0, subset = ['total_quantity'])\n",
    "line_id_lrp = set(lrp_eval['line_id'].tolist())\n",
    "total_quantity = []\n",
    "\n",
    "for i in range(len(line_id)):\n",
    "    if line_id[i] in line_id_lrp:\n",
    "        info = lrp_eval[lrp_eval['line_id']==line_id[i]]\n",
    "        total_quantity.append(info['total_quantity'].tolist()[0])\n",
    "    else:\n",
    "        total_quantity.append(0)\n",
    "\n",
    "eval_data['total_quantity'] = total_quantity\n",
    "\n",
    "line_id_lrp_enrol = lrp_enrollment_eval['line_id'].tolist()\n",
    "line_id_lrp_enrol = list(set(line_id_lrp_enrol))\n",
    "lrp_enrolled = []\n",
    "for i in range(len(line_id)):\n",
    "    if line_id[i] in line_id_lrp_enrol:\n",
    "        lrp_enrolled.append(1)\n",
    "    else:\n",
    "        lrp_enrolled.append(0)\n",
    "eval_data['lrp_enrolled'] = lrp_enrolled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_features = eval_data.drop(columns=['line_id','date_observed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please pip install xgboost befor using this module\n",
    "import xgboost as xgb\n",
    "model_xgb = xgb.XGBClassifier(n_estimators = 6000,\n",
    "                              learning_rate=0.01,\n",
    "                              max_depth=10, \n",
    "                              min_child_weight=1,\n",
    "                              subsample=0.9,\n",
    "                              gamma=0.2,\n",
    "                              colsample_bytree=0.7,\n",
    "                              objective= 'binary:logistic',\n",
    "                              nthread=4,\n",
    "                              scale_pos_weight=1,\n",
    "                              seed=27,\n",
    "                              reg_alpha=1e-05,\n",
    "                              use_label_encoder = False)\n",
    "model_xgb.fit(df_X, df_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_prediction = model_xgb.predict(eval_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_id = upgrades_eval['line_id'].tolist()\n",
    "predictions=pd.DataFrame(eval_id,columns=['line_id'])\n",
    "predictions['prediction']=eval_prediction\n",
    "submission_path=root_folder+\"submission/2021-04-25.csv\"\n",
    "predictions.to_csv(submission_path,header=True,index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
